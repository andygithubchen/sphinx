#
# Sphinx configuration file sample
#
# WARNING! While this sample file mentions all available options,
# it contains (very) short helper descriptions only. Please refer to
# doc/sphinx.html for details.
#

#############################################################################
## 定义数据源
#############################################################################

source src1
{
  #数据库相关配置
	type     = mysql
	sql_host = localhost
	sql_user = test
	sql_pass =
	sql_db   = test
	sql_port = 3306
	#sql_sock				= /tmp/mysql.sock #如果是使用unix sock连接可以使用这个。


## indexer和mysql之间的交互，需要考虑到效率和安全性。
## 比如考虑到效率，他们两者之间的交互需要使用压缩协议；考虑到安全，他们两者之间的传输需要使用ssl
## 那么这个参数就代表这个意思，0/32/2048/32768  无/使用压缩协议/握手后切换到ssl/Mysql 4.1版本身份认证。
	# mysql_connect_flags	= 32

# 当mysql_connect_flags设置为2048（ssl）的时候，下面几个就代表ssl连接所需要使用的几个参数。
	# mysql_ssl_cert		= /etc/ssl/client-cert.pem
	# mysql_ssl_key		= /etc/ssl/client-key.pem
	# mysql_ssl_ca		= /etc/ssl/cacert.pem

	# mssql_winauth			= 1 #mssql特有，是否使用windows登陆

	# mssql_unicode			= 1 #mssql特有，是使用unicode还是单字节数据。


	# odbc_dsn				= DBQ=C:\data;DefaultDir=C:\data;Driver={Microsoft Text Driver (*.txt; *.csv)}; #odbc的dsn串
	# sql_query				= SELECT id, data FROM documents.csv

  #indexer的sql执行前需要执行的操作。
	# sql_query_pre			= SET NAMES utf8
	# sql_query_pre			= SET SESSION query_cache_type=OFF

	sql_query				= SELECT id, group_id, UNIX_TIMESTAMP(date_added) AS date_added, title, content FROM documents # 主文档获取查询, 强制选项，整数文档ID字段必须在第一个位置


## 当数据源数据太大的时候，一个sql语句查询下来往往很有可能锁表等操作。
#    ## 那么我么就可以使用多次查询，那么这个多次查询就需要有个范围和步长，sql_query_range和sql_range_step就是做这个使用的。
#        ## 获取最大和最小的id，然后根据步长来获取数据。比如下面的例子，如果有4500条数据，这个表建立索引的时候就会进行5次sql查询。 
#            ## 而5次sql查询每次的间隔时间是使用sql_ranged_rhrottle来进行设置的。单位是毫秒。
# sql_query				= SELECT doc.id, doc.id AS group, doc.title, doc.data FROM documents doc	WHERE id>=$start AND id<=$end
# sql_query_range		= SELECT MIN(id),MAX(id) FROM documents
# sql_range_step		= 1000 # 分区查询的步长，可选，默认是1024


# 无符号整型声明(可选)
# 可以分开指定多个值，准许任意数量的属性
# 位大小是可以被指定的，默认为32
	# sql_attr_uint			= author_id
	# sql_attr_uint			= forum_id:9 #存储9位给forum_id这个字段
	sql_attr_uint			= group_id


# 布尔属性声明(可选)
# 可以分开指定多个值，准许任意数量的属性
#相当于 sql_sttr_uint 给1位的长度
	# sql_attr_bool			= is_deleted


  #大整型属性声明(可选)
# 可以分开指定多个值，准许任意数量的属性
# 声明一个64位属性的签名
	# sql_attr_bigint			= my_bigint_id


# UNIX时间戳属性声明
# 可以分开指定多个值，准许任意数量的属性
# 类似于整数，但也可以使用日期函数
	# sql_attr_timestamp	= posted_ts
	# sql_attr_timestamp	= last_edited_ts
	sql_attr_timestamp		= date_added



    ## 字符串排序属性。一般我们按照字符串排序的话，我们会将这个字符串存下来进入到索引中，然后在查询的时候比较索引中得字符大小进行排序。
    #    ## 但是这个时候索引就会很大，于是我们就想到了一个方法，我们在建立索引的时候，先将字符串值从数据库中取出，暂存，排序。
    #        ## 然后给排序后的数组分配一个序号，然后在建立索引的时候，就将这个序号存入到索引中去。这样在查询的时候也就能完成字符串排序的操作。
    #            ## 这，就是这个字段的意义。
	# sql_attr_str2ordinal	= author_name


# 浮点属性声明
# 可以分开指定多个值，准许任意数量的属性
# 值以单精度32位IEEE 754格式存储
	# sql_attr_float = lat_radians
	# sql_attr_float = long_radians


# 多值属性（MVA）属性声明
# 可以分开指定多个值，准许任意数量的属性
#         ## 试想一下，有一个文章系统，每篇文章都有多个标签，这个文章就叫做多值属性。
#             ## 我要对某个标签进行查询过滤，那么在建立查询的时候就应该把这个标签的值放入到索引中。
#                 ## 这个字段，sql_attr_multi就是用来做这个事情的。
#
	# 语法： ATTR-TYPE ATTR-NAME 'from' SOURCE-TYPE [;QUERY] [;RANGE-QUERY]
	# ATTR-TYPE is 'uint' or 'timestamp'
	# SOURCE-TYPE is 'field', 'query', or 'ranged-query'
	# QUERY is SQL query used to fetch all ( docid, attrvalue ) pairs # QUERY是用于获取所有（docid，attrvalue）对的SQL查询
  # RANGE-QUERY is SQL query used to fetch min and max ID values, similar to 'sql_query_range' # RANGE-QUERY是用于获取最小和最大ID值的SQL查询，类似于'sql_query_range'
	#
	# sql_attr_multi	= uint tag from query; SELECT id, tag FROM tags
# sql_attr_multi	= uint tag from ranged-query;	SELECT id, tag FROM tags WHERE id>=$start AND id<=$end; SELECT MIN(id), MAX(id) FROM tags


    ## 取后查询，在sql_query执行后立即操作。
    #    ## 它和sql_query_post_index的区别就是执行时间不同
    #        ## sql_query_post是在sql_query执行后执行，而sql_query_post_index是在索引建立完成后才执行。
    #            ## 所以如果要记录最后索引执行时间，那么应该在sql_query_post_index中执行。
	# sql_query_post		=   

# post-index-query，成功建立索引后执行, 可选，默认为空
# $maxid扩展为实际从数据库获取的最大文档ID
	# sql_query_post_index = REPLACE INTO counters ( id, val ) VALUES ( 'max_indexed_id', $maxid )


	sql_ranged_throttle	= 0 # 设置分区查询的时间间隔, 可选，默认为0，表示没有延迟(以毫秒为单位)

    ## 命令行获取信息查询。
    #        ## 我们进行索引一般只会返回主键id，而不会返回表中的所有字段。
    #            ## 但是在调试的时候，我们一般需要返回表中的字段，那这个时候，就需要使用sql_query_info。
    #                ## 同时这个字段只在命令行下有效，在api中是无效的。
	sql_query_info		= SELECT * FROM documents WHERE id=$id


   ## 比如有两个索引，一个索引比较旧，一个索引比较新，那么旧索引中就会有数据是旧的。
   #    ## 当我要对两个索引进行搜索的时候，哪些数据要按照新的索引来进行查询呢。
   #        ## 这个时候就使用到了这个字段了。
	# sql_query_killlist	= SELECT id FROM documents WHERE edited>=@last_reindex


    ## 下面几个压缩解压的配置都是为了一个目的：让索引重建的时候不要影响数据库的性能表现。
	# unpack_zlib = zlib_column ## SQL数据源解压字段设置
	# unpack_mysqlcompress = compressed_column # MySQL数据源解压字段设置
	# unpack_mysqlcompress = compressed_column_2 # MySQL数据源解压字段设置


	# unpack_mysqlcompress_maxsize = 16M #MySQL数据源解压缓冲区设置


	#####################################################################
	## xmlpipe settings
	#####################################################################

	# type				= xmlpipe ## xmlpipe的数据源就是一个xml文档
	# xmlpipe_command	= cat /usr/local/coreseek/var/test.xml #读取数据源的命令

	#####################################################################
	## xmlpipe2 settings
	#####################################################################

	# type				= xmlpipe2
	# xmlpipe_command	= cat /usr/local/coreseek/var/test2.xml


	# xmlpipe2字段声明,  多值，可选，默认为空列表
	# xmlpipe_field				= subject
	# xmlpipe_field				= content


	# xmlpipe2字段声明,  多值，可选，默认为空列表
	#所有xmlpipe_attr_XXX选项与sql_attr_XXX完全相似
	# xmlpipe_attr_timestamp	= published
	# xmlpipe_attr_uint			= author_id

    ## UTF-8修复设置
    #    ## 只适用xmlpipe2数据源，数据源中有可能有非utf-8的字符，这个时候解析就有可能出现问题
    #        ## 如果设置了这个字段，非utf-8序列就会全部被替换为空格。
	# xmlpipe_fixup_utf8		= 1
}


# 继承源代码示例, 所有参数都是从父源复制的，然后可以在此源定义中被覆盖
source src1throttled : src1
{
	sql_ranged_throttle			= 100
}

#############################################################################
## 索引定义
#############################################################################

index test1
{
	source			= src1 # 数据源名称，必填，且在数据源的配置中只能有一个

	path			= /usr/local/coreseek/var/data/test1 # 索引文件路径和文件名，不带扩展名, 必填，路径必须是可写的，扩展名会自动生成

  ## 文档信息的存储模式，包括有none,extern,inline。默认是extern。
  #    ## docinfo指的就是数据的所有属性（field）构成的一个集合。
  #        ## 首先文档id是存储在一个文件中的（spa）
  #            ## 当使用inline的时候，文档的属性和文件的id都是存放在spa中的，所以进行查询过滤的时候，不需要进行额外操作。
  #                ## 当使用extern的时候，文档的属性是存放在另外一个文件（spd）中的，但是当启动searchd的时候，会把这个文件加载到内存中。
  #                    ## extern就意味着每次做查询过滤的时候，除了查找文档id之外，还需要去内存中根据属性进行过滤。
  #                        ## 但是即使这样，extern由于文件大小小，效率也不低。所以不是有特殊要求，一般都是使用extern
	docinfo			= extern 

    ## 缓冲内存锁定。
    #    ## searchd会讲spa和spi预读取到内存中。但是如果这部分内存数据长时间没有访问，则它会被交换到磁盘上。
    #        ## 设置了mlock就不会出现这个问题，这部分数据会一直存放在内存中的。
	mlock			= 0



# 应用的形态预处理器列表, 可选，默认为空
# 内置的预处理器是'none'，'stem_en'，'stem_ru'，'stem_enru', 'soundex'和'metaphone';
# libstemmer提供的其他预处理程序是“libstemmer_XXX”，其中XXX是算法代码, （请参阅libstemmer_c / libstemmer / modules.txt）
    ## 词形处理器
    #    ## 词形处理是什么意思呢？比如在英语中，dogs是dog的复数，所以dog是dogs的词干，这两个实际上是同一个词。
    #        ## 所以英语的词形处理器会讲dogs当做dog来进行处理。
	#
	# morphology 	= stem_en, stem_ru, soundex
	# morphology	= libstemmer_german
	# morphology	= libstemmer_sv
	morphology		= none

  #词形处理有的时候会有问题，比如将gps处理成gp，这个设置可以允许根据词的长度来决定是否要使用词形处理器。
	# 可选，默认为1
	# min_stemming_len	= 1


#停止词，停止词是不被索引的词。
	# 可选，默认为空
	#stopwords			= G:\data\stopwords.txt


#自定义词形字典, 可选，默认为空
	#wordforms			= G:\data\wordforms.txt



  ## 词汇特殊处理。
  #    ## 有的一些特殊词我们希望把它当成另外一个词来处理。比如，c++ => cplusplus来处理。
	# 可选，默认为空
	#exceptions		= /data/exceptions.txt


	# 最小索引词长度，小于这个长度的词不会被索引。
	# 默认为1
	min_word_len		= 1

  # 字符集编码类型，可以为sbcs,utf-8。对于Coreseek，还可以有zh_cn.utf-8,zh_ch.gbk,zh_ch.big5
	charset_type		= sbcs

#字符表和大小写转换规则。对于Coreseek，这个字段无效。
	# 默认为'sbcs'
	# charset_table		= 0..9, A..Z->a..z, _, a..z, U+A8->U+B8, U+B8, U+C0..U+DF->U+E0..U+FF, U+E0..U+FF
	#
	# 默认为'utf-8'
	# charset_table		= 0..9, A..Z->a..z, _, a..z, U+410..U+42F->U+430..U+44F, U+430..U+44F


  # 忽略字符表。在忽略字符表中的前后词会被连起来当做一个单独关键词处理。
	# 可选，默认为空
	# ignore_chars		= U+00AD


#min_prefix_len,min_infix_len,prefix_fields,infix_fields都是在enable_star开启的时候才有效果。
   ## 最小前缀索引长度
   #    ## 为什么要有这个配置项呢？
   #        ## 首先这个是当启用通配符配置启用的前提下说的，前缀索引使得一个关键词产生了多个索引项，导致索引文件体积和搜索时间增加巨大。
   #            ## 那么我们就有必要限制下前缀索引的前缀长度，比如example，当前缀索引长度设置为5的时候，它只会分解为exampl，example了。
   #
	# min_prefix_len	= 0 


  #最小索引中缀长度。理解同上。
	# min_infix_len		= 0


  # 前缀索引和中缀索引字段列表。并不是所有的字段都需要进行前缀和中缀索引。
	# prefix_fields		= filename
	# infix_fields		= url, domain


#是否启用通配符，默认为0，不启用
	# enable_star		= 1


#    ## N-Gram索引的分词技术
#        ## N-Gram是指不按照词典，而是按照字长来分词，这个主要是针对非英文体系的一些语言来做的（中文、韩文、日文）
#            ## 对coreseek来说，这两个配置项可以忽略。
	# ngram_len				= 1
	# ngram_chars			= U+3000..U+2FA1F


    ## 词组边界符列表和步长
    #    ## 哪些字符被看做分隔不同词组的边界。
	# phrase_boundary		= ., ?, !, U+2026 # horizontal ellipsis
	# phrase_boundary_step	= 100


#html标记清理，是否从输出全文数据中去除HTML标记。0/1
	html_strip				= 0

#HTML标记属性索引设置。
	# html_index_attrs		= img=alt,title; a=title;

#需要清理的html元素
	# html_remove_elements	= style, script

#searchd是预先打开全部索引还是每次查询再打开索引。0/1
	# preopen					= 1

#字典文件是保持在磁盘上还是将他预先缓冲在内存中。0/1
	# ondisk_dict				= 1

    ## 由于在索引建立的时候，需要建立临时文件和和副本，还有旧的索引
    #    ## 这个时候磁盘使用量会暴增，于是有个方法是临时文件重复利用
    #        ## 这个配置会极大减少建立索引时候的磁盘压力，代价是索引建立速度变慢。
	# inplace_enable			= 1 # optional, default is 0 (use separate temporary files), indexer-only
	# inplace_hit_gap			= 0		# preallocated hitlist gap size
	# inplace_docinfo_gap		= 0		# preallocated docinfo gap size
	# inplace_reloc_factor	= 0.1	# relocation buffer size within arena
	# inplace_write_factor	= 0.1	# write buffer size within arena


	#词形处理后是否还要检索原词？0/1
	# index_exact_words		= 1


#在经过过短的位置后增加位置值 0/1
	# overshort_step			= 1


  # 在经过 停用词 处后增加位置值 0/1
	# stopword_step			= 1
}


# test1stemmed 继承 test1 索引
index test1stemmed : test1
{
	path			= /usr/local/coreseek/var/data/test1stemmed
	morphology		= stem_en
}


#分布式索引示例
#
#这是一个不能直接索引的虚拟索引，
#并且仅包含对其他本地和/或远程索引的引用
index dist1
{
  # 索引类型，包括有plain，distributed和rt。分别是普通索引/分布式索引/增量索引。默认是plain。
	type				= distributed

#要搜索的本地索引
#可以配置很多本地索引
	local				= test1
	local				= test1stemmed

#分布式索引（distributed index）中的远程代理和索引声明
	# syntax for TCP connections is 'hostname:port:index1,[index2[,...]]'
	# syntax for local UNIX connections is '/path/to/socket:index1,[index2[,...]]'
	agent				= localhost:9313:remote1
	agent				= localhost:9314:remote2,remote3
	# agent				= /var/run/searchd.sock:remote4

	# 分布式索引（ distributed index）中声明远程黑洞代理
	# 网络错误时搜索结果将被忽略
	# agent_blackhole		= testbox:9312:testindex1,testindex2


	#远程代理的连接超时时间
	agent_connect_timeout	= 1000

	#远程查询超时时间
	agent_query_timeout		= 3000
}

index rt
{
  type            = rt

    path            = /home/yejianfeng/instance/coreseek/var/data/rt

## RT索引内存限制
# rt_mem_limit      = 512M

## 全文字段定义
    rt_field        = title
    rt_field        = content

## 无符号整数属性定义
    rt_attr_uint        = gid

## 各种属性定义
# rt_attr_bigint        = guid
# rt_attr_float     = gpa
# rt_attr_timestamp = ts_added
# rt_attr_string        = author
}


#############################################################################
## indexer settings
#############################################################################

indexer
{
	# 建立索引的时候，索引内存限制
  # 可选，默认为32M，最大为2047M，推荐256M至1024M
	mem_limit			= 32M

	#每秒最大I/O操作次数，用于限制I/O操作
	# max_iops			= 40


	#最大允许的I/O操作大小，以字节为单位，用于I/O节流
	# max_iosize		= 1048576


	# 对于XMLLpipe2数据源允许的最大的字段大小，以字节为单位
	# max_xmlpipe2_field	= 4M


	#写缓冲区的大小，单位是字节
	# write_buffer		= 1M
}

#############################################################################
## 搜索服务配置
#############################################################################

searchd
{
  # hostname，port或hostname:port或/unix/socket/path进行监听
  # 多值，允许多个监听点
  # 可选，默认为0.0.0.0:9312（侦听所有接口，端口9312）
	#
	# listen				= 127.0.0.1
	# listen				= 192.168.0.1:9312
	# listen				= 9312
	# listen				= /var/run/searchd.sock


	#监听日志, 可选，默认为'searchd.log'
	log					= /usr/local/coreseek/var/log/searchd.log

# 查询日志文件，所有搜索查询都记录在这里 , 可选，默认为空（不记录查询）
	query_log			= /usr/local/coreseek/var/log/query.log

  # 客户端读取超时， 可选，默认为5秒
	read_timeout		= 5

  # 请求超时，秒, 可选，默认为5分钟
	client_timeout		= 300

#并行执行搜索的数目, 可选，默认为0（无限制）
	max_children		= 30

  # searchd进程PID路径和文件名, 强制性
	pid_file			= /usr/local/coreseek/var/log/searchd.pid

#守护进程在内存中为每个索引所保持并返回给客户端的匹配数目的最大值
	max_matches			= 1000

#    ## 无缝轮转。防止 searchd 轮换在需要预取大量数据的索引时停止响应
#        ## 当进行索引轮换的时候，可能需要消耗大量的时间在轮换索引上。
#            ## 但是启动了无缝轮转，就以消耗内存为代价减少轮转的时间
	seamless_rotate		= 1

  # 索引预开启，是否强制重新打开所有索引文件, 0/1
	preopen_indexes		= 0

#索引轮换成功之后，是否删除以.old为扩展名的索引拷贝, 0/1
	unlink_old			= 1

## 属性刷新周期
## 就是使用UpdateAttributes()更新的文档属性每隔多少时间写回到磁盘中。
	# attr_flush_period	= 900


	# 索引字典存储方式
	# ondisk_dict_default	= 1


  # 用于多值属性MVA更新的存储空间的内存共享池大小
	#可选，默认大小为1M
	mva_updates_pool	= 1M

  # 网络通讯时允许的最大的包的大小
	#可选，默认大小为8M
	max_packet_size		= 8M

# 崩溃日志路径
# searchd将（尝试）将崩溃的查询记录到“crash_log_path.PID”文件
# 可选，默认为空（不要创建崩溃日志）
	# crash_log_path		= /usr/local/coreseek/var/log/crash


	# 每次查询允许设置的过滤器的最大个数
	#可选，默认大小为256
	max_filters			= 256

  # 单个过滤器允许的值的最大个数
	#可选，默认大小为4096
	max_filter_values	= 4096


  # TCP监听待处理队列长度
	#可选，默认大小为5
	# listen_backlog		= 5


  # 每个关键字的读缓冲区的大小
	#可选，默认大小为256k
	# read_buffer			= 256K


  # 无匹配时读操作的大小
	#可选，默认大小为32k
	# read_unhinted		= 32K
}

# --eof--
